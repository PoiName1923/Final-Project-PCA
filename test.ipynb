{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2033e9",
   "metadata": {},
   "source": [
    "## Test của Thuận"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    A class to represent a TF-IDF vectorizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _split_sentences(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Split the text into sentences.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be split into sentences.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of sentences.\n",
    "        \"\"\"\n",
    "\n",
    "        # Cut text into sentences\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "    def _preprocess_text(self, sentences_list: list) -> Tuple[list, set, dict, dict, int]:\n",
    "        \"\"\"\n",
    "        Preprocess the text:\n",
    "            - Convert to lowercase.\n",
    "            - Remove punctuation.\n",
    "\n",
    "        Returns:\n",
    "            sentences (list): List of tokenized sentences.\n",
    "            word_set (set): Set of unique words.\n",
    "            word_count (dict): Dictionary with words as keys and their document frequency as values.\n",
    "            word_dict (dict): Dictinary with words as keys and their indices as values.\n",
    "            total_documents (int): Number of sentences.\n",
    "        \"\"\"\n",
    "\n",
    "        sentences = []\n",
    "        word_set = []\n",
    "        \n",
    "        for sent in sentences_list:\n",
    "            words = [word.lower() for word in word_tokenize(sent) if word.isalpha()]\n",
    "            sentences.append(words)\n",
    "            for word in words:\n",
    "                if word not in word_set:\n",
    "                    word_set.append(word)\n",
    "\n",
    "        word_set = set(word_set)\n",
    "\n",
    "        total_documents = len(sentences)\n",
    "        \n",
    "        word_dict = {}\n",
    "        word_dict = {word: i for i, word in enumerate(word_set)}\n",
    "\n",
    "\n",
    "        word_count = {}\n",
    "        for word in word_set:\n",
    "            word_count[word] = 0\n",
    "            for sent in sentences:\n",
    "                if word in sent:\n",
    "                    word_count[word] += 1\n",
    "                    \n",
    "        return sentences, word_set, word_count, word_dict, total_documents\n",
    "    \n",
    "\n",
    "    \n",
    "    def _term_freq(self, words: list, word: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate TF (Term Frequency) for the preprocessed text.\n",
    "\n",
    "        Args:\n",
    "            words (list): List of words in the sentence.s\n",
    "            word (str): The word to calculate TF.\n",
    "\n",
    "        Returns:\n",
    "            float: the TF value.\n",
    "        \"\"\"\n",
    "\n",
    "        N = len(words)\n",
    "        occurance = len([token for token in words if token == word])\n",
    "\n",
    "        return occurance / N\n",
    "    \n",
    "    \n",
    "    def _inverse_doc_freq(self, word: str, word_count: dict, total_documents: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate IDF (Inverse Document Frequency) for the given word.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to calculate IFD.\n",
    "            word_count (dict): Dictionary with words as keys and their doc frequency as values.\n",
    "            total_documents (int): Total number of sentences.\n",
    "\n",
    "        Returns:\n",
    "            float: The IDF value for the word.\n",
    "        \"\"\"\n",
    "        word_occurrence = word_count.get(word) \n",
    "\n",
    "        idf = np.log(total_documents / word_occurrence)\n",
    "\n",
    "        return idf\n",
    "    \n",
    "\n",
    "    def _tf_idf(self, sentence: list, word_set: set, word_count: dict, \n",
    "                     word_dict: dict, total_documents: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the TF-IDF vector.\n",
    "\n",
    "        Args:\n",
    "            sentence (list): List of words in the sentence.\n",
    "            word_set (set): Set of unique words.\n",
    "            word_count (dict): Dictionary with words as keys and their document frequency as values.\n",
    "            word_dict (dict): Dictionary with words as keys and their indices as values.\n",
    "            total_documents (int): Total number of sentences.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The TF-IDF vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        tf_idf_vec = np.zeros((len(word_set),))\n",
    "\n",
    "        for word in sentence:\n",
    "            tf = self._term_freq(sentence, word)\n",
    "            idf = self._inverse_doc_freq(word, word_count, total_documents)\n",
    "            \n",
    "            value = tf * idf\n",
    "            tf_idf_vec[word_dict[word]] = value \n",
    "\n",
    "        return tf_idf_vec\n",
    "    \n",
    "\n",
    "    def transform(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implement the TF-IDF vectorization process.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The TF-IDF vector for the text.\n",
    "        \"\"\"\n",
    "\n",
    "        #Split text into sentences\n",
    "        sentences_list = self._split_sentences(text)\n",
    "\n",
    "        # Preprocess text and calculate TF-IDF vectors\n",
    "        vectors = []\n",
    "        sentences, word_set, word_count, word_dict, total_documents = self._preprocess_text(sentences_list)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tf_idf_vec = self._tf_idf(sentence, word_set, word_count, word_dict, total_documents)\n",
    "            vectors.append(tf_idf_vec)\n",
    "        \n",
    "        return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class FeatureVectorizer:\n",
    "    \"\"\"\n",
    "    A class to vectorize text, arrays, dataframes to a feature vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def _text_vectorizer(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Vectorize text data into a feature vector.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array representing the feature vector of the text data.\n",
    "        \"\"\"\n",
    "        return TfidfVectorizer().transform(text)\n",
    "\n",
    "\n",
    "    def _image_vectorizer(self, image_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Vectorize image data into a feature vector.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: An array representing the feature vector of the image data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check params\n",
    "        if not isinstance(image_matrix, np.ndarray):\n",
    "            raise TypeError(\"image_matrix must be a numpy array.\")\n",
    "        \n",
    "        # if image_matrix.shape != (64, 64):\n",
    "        #     raise ValueError(\"image_matrix must be of shape (64, 64).\")\n",
    "\n",
    "        plt.imshow(image_matrix, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(\"test_output.png\")   # Lưu ảnh\n",
    "        plt.show()\n",
    "\n",
    "        # row, col = image_matrix.shape\n",
    "\n",
    "        # flat_array = np.zeros((row * col,))\n",
    "\n",
    "        # idx = 0\n",
    "\n",
    "        # for i in range(row):\n",
    "        #     for j in range(col):\n",
    "        #         flat_array[idx] = image_matrix[i][j]\n",
    "        #         idx += 1\n",
    "            \n",
    "        # return flat_array      # shape (row * col,)\n",
    "    \n",
    "\n",
    "    def _table_vectorizer(self, table_data: pd.DataFrame, length_threshold: int = 50) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize and vectorize table data into a feature vector.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array representing the feature vector of the table data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check params\n",
    "        if not isinstance(table_data, pd.DataFrame):\n",
    "            raise TypeError(\"table_data must be a pandas DataFrame.\")\n",
    "        \n",
    "        # Normalize numeric columns\n",
    "        for numeric_col in table_data.select_dtypes(include = ['number']).columns:\n",
    "            mean = table_data[numeric_col].mean()\n",
    "            std  = table_data[numeric_col].std()\n",
    "            table_data[numeric_col] = (table_data[numeric_col] - mean) / std\n",
    "\n",
    "        # Columns to drop (comment, long text, ...)\n",
    "        cols_to_drop = []\n",
    "\n",
    "        # Label Encoding for text columns\n",
    "        for text_col in table_data.select_dtypes(include = ['object']).columns:\n",
    "            if isinstance(table_data[text_col], pd.DataFrame):\n",
    "                print(table_data)\n",
    "            avg_length = table_data[text_col].fillna('').apply(lambda x: len(str(x))).mean()\n",
    "            unique_values = list(table_data[text_col].unique())\n",
    "\n",
    "            # Check:\n",
    "            # 1. If the average length of the text is below the threshold\n",
    "            # (Avoid comment columns, long text columns, ...)\n",
    "            # 2. If the number of unique values is less than half of the total number of rows\n",
    "            # (Avoid columns with too many unique values, like names, ...)\n",
    "            if avg_length < length_threshold and len(unique_values) < len(table_data) / 2: \n",
    "                table_data[text_col] = table_data[text_col].\\\n",
    "                                       apply(lambda x: unique_values.index(x)).fillna(-1).astype('int')\n",
    "            \n",
    "            # If the average length is above the threshold or the number of unique values is too high, remove the column\n",
    "            else:\n",
    "                cols_to_drop.append(text_col)\n",
    "        \n",
    "        # Drop long text columns\n",
    "        normalized_data = table_data.drop(columns = cols_to_drop)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        normalized_data = normalized_data.to_numpy()\n",
    "\n",
    "        return normalized_data\n",
    "    def vectorize(self, data_info: dict) -> np.ndarray:\n",
    "        print(\"AAA\")\n",
    "        \"\"\"\n",
    "        Vectorize the data based on its type.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: An array representing the feature vector of the data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check params\n",
    "        if not isinstance(data_info, dict):\n",
    "            raise TypeError(\"data_info must be a dict.\")\n",
    "        \n",
    "        if not all(key in data_info for key in ['type', 'content', 'meta']):\n",
    "            raise ValueError(\"data_info must contain 'type', 'content', and 'meta' keys.\")\n",
    "\n",
    "        # Check params\n",
    "        if data_info['type'] not in ['mutiple', 'text', 'image', 'table']:\n",
    "            raise ValueError(\"data_type must be one of 'text', 'image', 'table'.\")\n",
    "        \n",
    "        data_type = data_info['type']\n",
    "        content   = data_info['content']\n",
    "        metadata  = data_info['meta']\n",
    "        \n",
    "        # Handle different data types\n",
    "        if data_type == 'mutiple':\n",
    "            vectors = []\n",
    "            # if content['text'] is not None:\n",
    "            #     vectors.append(self._text_vectorizer(content['text']))\n",
    "            \n",
    "            if content['images'] is not None:\n",
    "                for image in content['images']:\n",
    "                    vectors.append(self._image_vectorizer(image))\n",
    "            \n",
    "            # if content['tables'] is not None:\n",
    "            #     for table in content['tables']:\n",
    "            #         vectors.append(self._table_vectorizer(table))\n",
    "            \n",
    "            return vectors\n",
    "        \n",
    "        # Handle text data type\n",
    "        elif data_type == 'text':\n",
    "            return self._text_vectorizer(content)\n",
    "        \n",
    "        # Handle image data type\n",
    "        elif data_type == 'image':\n",
    "            return self._image_vectorizer(content)\n",
    "        \n",
    "        # Handle table data type\n",
    "        else: return self._table_vectorizer(content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ca47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.Reader.reader_agent import DataExpander\n",
    "\n",
    "# Path to the files\n",
    "txt_path   = os.path.join(\"data_test_for_reader\", \"report.txt\")\n",
    "image_path = os.path.join(\"data_test_for_reader\", \"cat.jpg\")\n",
    "table_path = os.path.join(\"data_test_for_reader\", \"Network.csv\")\n",
    "pdf_path   = os.path.join(\"data_test_for_reader\", \"test_pdf.pdf\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 0. Call modules\n",
    "    reader = DataExpander()\n",
    "    vectorizer = FeatureVectorizer()\n",
    "\n",
    "\n",
    "    # 1. Read files\n",
    "    #text_data = dict(reader.expand(txt_path)[0])\n",
    "    #image_data = dict(reader.expand(image_path)[0])\n",
    "    #table_data = dict(reader.expand(table_path)[0])\n",
    "    pdf_data = dict(reader.expand(pdf_path)[0])\n",
    "\n",
    "    # 2. Vectorize data\n",
    "    #text_vector = vectorizer.vectorize(text_data)\n",
    "    #image_vector = vectorizer.vectorize(image_data)\n",
    "    #table_vector = vectorizer.vectorize(table_data)\n",
    "    # pdf_vector = vectorizer.vectorize(pdf_data)\n",
    "\n",
    "    # 3. Print results\n",
    "    #print(\"text after vectorization: \\n\", text_vector)\n",
    "    #print(\"image after vectorization:\", image_vector)\n",
    "    # print(\"table after vectorization: \\n\", table_vector)\n",
    "    #print(\"pdf after vectorization: \\n\", pdf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65511d",
   "metadata": {},
   "source": [
    "## Test của Tiến"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff90ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.Reader.reader_agent import DataExpander\n",
    "\n",
    "# Path to the files\n",
    "txt_path   = os.path.join(\"data_test_for_reader\", \"report.txt\")\n",
    "image_path = os.path.join(\"data_test_for_reader\", \"cat.jpg\")\n",
    "table_path = os.path.join(\"data_test_for_reader\", \"Network.csv\")\n",
    "pdf_path   = os.path.join(\"data_test_for_reader\", \"test_pdf.pdf\")\n",
    "json_path   = os.path.join(\"data_test_for_reader\", \"data.json\")\n",
    "audio_path = os.path.join(\"data_test_for_reader\", \"music.mp3\")\n",
    "\n",
    "\n",
    "reader = DataExpander()\n",
    "# vectorizer = FeatureVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abcfb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "data = reader.expand(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c49d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data[0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3388681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to\n",
      "Machine Learning\n",
      "1\n",
      "Introduction\n",
      "2\n",
      "What is machine learning?\n",
      "For many problems, it’s difficult to program the correct behavior by hand\n",
      "• recognizing people and objects\n",
      "• understanding human speech\n",
      "Machine learning approach: program an algorithm to automatically learn\n",
      "from data, or from experience\n",
      "Some reasons you might want to use a learning algorithm:\n",
      "• hard to code up a solution by hand (e.g. vision, speech)\n",
      "• system needs to adapt to a changing environment (e.g. spam detection)\n",
      "• want the system to perform better than the human programmers\n",
      "• privacy/fairness (e.g. ranking search results)\n",
      "3\n",
      "What is machine learning?\n",
      "Types of machine learning\n",
      "• Supervised learning: have labeled examples of the correct behavior\n",
      "• Reinforcement learning: learning system receives a reward signal, tries\n",
      "to learn to maximize the reward signal\n",
      "• Unsupervised learning: no labeled examples – instead, looking for\n",
      "interesting patterns in the data\n",
      "4\n",
      "What is machine learning?\n",
      "Supervised learning: have labeled examples of the correct behavior\n",
      "e.g. Handwritten digit classification with the MNIST dataset\n",
      "• Task: given an image of a handwritten digit, predict the digit class\n",
      "o Input: the image\n",
      "o Target: the digit class\n",
      "• Data: 70,000 images of handwritten digits labeled by humans\n",
      "o Training set: first 60,000 images, used to train the network\n",
      "o Test set: last 10,000 images, not available during training, used\n",
      "to evaluate performance\n",
      "• This dataset is the “fruit fly” of neural net research\n",
      "• Neural nets already achieved > 99% accuracy in the 1990s, but we still\n",
      "continue to learn a lot from it\n",
      "5\n",
      "Supervised learning examples\n",
      "What makes a “2”?\n",
      "6\n",
      "Supervised learning examples\n",
      "Caption generation\n",
      "G: global image feature; SA: spatial attention; OA: object attention.\n",
      "7\n",
      "Supervised learning examples\n",
      "Caption generation\n",
      "8\n",
      "Supervised learning examples\n",
      "Caption generation\n",
      "Image captioning with attention\n",
      "9\n",
      "Unsupervised learning examples\n",
      "• K-means clustering\n",
      "K-means clustering iterations\n",
      "10\n",
      "Generative model\n",
      "• In generative modeling, we want to learn a distribution over some dataset, such\n",
      "as natural images.\n",
      "Generative Adversarial Network (GAN)\n",
      "11\n",
      "Generative model\n",
      "Generative Adversarial Network (GAN)\n",
      "Loss\n",
      "Update D\n",
      "Update G\n",
      "12\n",
      "Generative model\n",
      "Generative Adversarial Network (GAN)\n",
      "13\n",
      "Generative model\n",
      "Recent exciting result: a model called the CycleGAN takes lots of images of one\n",
      "category (e.g. horses) and lots of images of another category (e.g. zebras) and learns\n",
      "to translate between them.\n",
      "CycleGAN\n",
      "14\n",
      "Deep learning\n",
      "Deep learning: many layers (stages) of processing\n",
      "E.g. this network can recognize and segment objects in images\n",
      "Mask R-CNN\n",
      "17\n",
      "Deep learning\n",
      "Deep learning: many layers (stages) of processing\n",
      "E.g. the network can recognize and segments objects in images\n",
      "18\n",
      "What are neural networks?\n",
      "Why neural nets?\n",
      "• Inspiration from the brain: proof of concept that a neural architecture can see\n",
      "and hear!\n",
      "• Very effective across a range of applications (vision, text, speech, medicine,\n",
      "robotics, etc.)\n",
      "• Widely used in both academia and the tech industry\n",
      "• Powerful software frameworks let us quickly implement sophisticated\n",
      "algorithms\n",
      "19\n",
      "Software frameworks\n",
      "Array processing (NumPy)\n",
      "• Vectorize computations (express them in terms of matrix/vector operations) to\n",
      "exploit hardware efficiency\n",
      "Neural net frameworks:\n",
      "• Automatic differentiation\n",
      "• Compiling computation graphs\n",
      "• Libraries of algorithms and network primitives\n",
      "• Support for graphics processing units (GPUs)\n",
      "Book for this course:\n",
      "• Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville\n",
      "http://www.deeplearningbook.org/\n",
      "20\n",
      "Q&A\n",
      "Thank you\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
